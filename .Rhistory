ggplot(aes(x = exp.content.ng.perML, y = accuracy, color = compounds)) +
geom_segment(aes(x = 0, xend = df.cal.accuracy.tidy$exp.content.ng.perML %>% max(),
y = 100, yend = 100),
linetype = "dashed", size = .2, color = "black") +
annotate(geom = "rect", xmin = 0, xmax = df.cal.accuracy.tidy$exp.content.ng.perML %>% max(),
ymin = 90, ymax = 110, fill = "dark green", alpha = .1) +
geom_point(size = .5, alpha = .8) +
scale_x_log10() +
annotation_logticks(sides = "b") +
scale_y_continuous(limits = c(0, 200), breaks = seq(0, 200, 20)) +
theme(legend.position = "None", title = element_text(face = "bold")) +
ggtitle("Calibration accuracy") +
scale_color_manual(values = AA.colors)
plt.cal.accuracy
# Statistical analysis and visualizaiton
# Calibration accuracy visualization
plt.cal.accuracy = df.cal.accuracy.tidy %>%
ggplot(aes(x = exp.content.ng.perML, y = accuracy, color = compounds)) +
geom_segment(aes(x = 0, xend = df.cal.accuracy.tidy$exp.content.ng.perML %>% max(),
y = 100, yend = 100),
linetype = "dashed", size = .2, color = "black") +
annotate(geom = "rect", xmin = 0, xmax = df.cal.accuracy.tidy$exp.content.ng.perML %>% max(),
ymin = 90, ymax = 110, fill = "dark green", alpha = .1) +
geom_point(size = .5, alpha = .8) +
scale_x_log10() +
annotation_logticks(sides = "b") +
scale_y_continuous(limits = c(0, 200), breaks = seq(0, 200, 20)) +
theme(legend.position = "None", title = element_text(face = "bold")) +
labs(title = "Calibration accuracy",
x = "Expected concentration (ng/mL)", y = "Accuracy (%)") +
scale_color_manual(values = AA.colors)
plt.cal.accuracy
df.dilutionError = df.cal.accuracy.tidy %>%
group_by(compounds, level) %>%
mutate(error.percent = abs((resp - mean(resp)) / mean(resp)) * 100) %>%  # normalize as percent relative to the mean at each level
summarise(error.percent.mean = mean(error.percent)) %>% # normalized response variance
ungroup() %>%
mutate(level = as.numeric(level),
level.max = max(level),
dilutionSteps = level.max -level)  # all levels uniformly converted to number of dilution steps
plt.dilutionError = df.dilutionError %>%
ggplot(aes(x = dilutionSteps, y = error.percent.mean, color = compounds)) +
geom_smooth(method = "lm", se = F, aes(group = 1), color = "black",
size = 5, alpha = .05) +
geom_smooth(method = "lm", se = F, aes(group = compounds))  +
geom_point() + geom_line(alpha = .2) +
scale_color_manual(values = AA.colors) +
scale_y_log10() + annotation_logticks(side = "l") +
theme(legend.position = "NA") +
labs(title = "Error propogation in calibration dilution steps",
y = "Error percent", x = "Dilution steps form stock solution (step 0)") +
# add amino acid label
geom_text(data = df.dilutionError %>% filter(dilutionSteps ==0),
aes(x = -0.5, label = compounds), size = 3)
plt.dilutionError
StepError = lm(error.percent.mean ~ dilutionSteps, data = df.dilutionError)  %>%
summary()
StepError
plot_grid(plt.cal.accuracy, plt.dilutionError, nrow = 1, align = "h")
#### PART III: Visualization of calibration curve
# import calibration intercept dataset (with 1/x weight)
df.cal.intercept = read_excel(path, sheet = "Calibration_intercept")
# augment calibration dataset with cal curve intercept with 1/x weighe
df.cal.accuracy.tidy = df.cal.accuracy.tidy %>%
left_join(df.cal.intercept, by = "compounds") %>%
# y = ax + b convert to y-b = ax, for visualization purpose
mutate(resp.subtractIntercept = resp - `intercept.1/x.weight`)
# plot
plt.calibrationCurve = df.cal.accuracy.tidy %>%
ggplot(aes(x = exp.content.ng.perML, y = resp.subtractIntercept, color = compounds)) +
geom_smooth(method = "lm", se = F, size = .5, color = "firebrick") +
geom_point(alpha = .6) +
facet_wrap(~compounds, scales = "free", nrow = 3) +
scale_x_log10() + scale_y_log10() + annotation_logticks() +
labs(caption = "Each level composed of 2~4 calibrators") +
scale_color_manual(values = AA.colors) +
labs(x = "Concentration (ng/mL)", y = "Response with intercept subtracted",
caption = "Intercept with 1/x weight was subtracted from peak response,
Both x and y scales are logarithmically transformed,
That is, what is plotted is not y = ax + b, but log(y-b) = log(ax) = log(a) + log(x)") +
theme(legend.position = "NA")
plt.calibrationCurve
# measured injecton concentration
df.inj.conc = read_excel(path, sheet = "validation injection conc.", range = "A1:X90")
df.inj.conc = df.inj.conc %>%
filter(!Sample %in% c("Accuracy_F_r3.d", "matrix effect_f_r2", "matrix effect_g_r1"))
# standard stock concentration
df.stock.conc = read_excel(path, sheet = "validation spike amount", range = "A1:B22")
# standard stock spike volume
df.spk.volume = read_excel(path, sheet = "validation spike amount", range = "A25:B32")
# Compute background.
# Note the concentration, ng/mL, track back to original extract, i.e., before 100-fold dilution
df.background = df.inj.conc %>% filter(Purpose == "Background") %>%
select(-c(Purpose, Sample, Level)) %>%
gather(key = compounds, value = background) %>%
group_by(compounds) %>%
summarise(
# background / background content mean level and dispersion
background.mean = mean(background * 100),
background.sd = sd(background * 100))
# injection concentration associated with accuracy computation
df.inj.conc.accuracy = df.inj.conc %>% filter(Purpose == "Accuracy") %>%
gather(-c(Purpose, Sample, Level), key = compounds, value = conc.inj)
df.inj.conc.accuracy
# Compute stats of the quality control sample (QC) spiked with standards
# Compute final concentration expected, and expected deviation from background
df.QC = (x = df.inj.conc.accuracy %>% select(Level, compounds))[!duplicated(x), ] %>% # compound-level combination
left_join(df.spk.volume, by = "Level") %>% # spike volume for different levels
mutate(plantExtractVol.uL = 800, # plant extract volume
# dilute factor after spiking
SpikeDiluteFactor = (plantExtractVol.uL + SpikeVol.uL)/SpikeVol.uL,
BackgroundDiluteFactor = (plantExtractVol.uL + SpikeVol.uL)/plantExtractVol.uL) %>%
left_join(df.background, by = "compounds") %>%
left_join(df.stock.conc, by = "compounds") %>%
mutate(
# the following three lines are the component-wise concentration with correction of dilution effect of spiking
# the concentration is that of QC, prior to 100-fold dilution;
# all three conc. marked as "QC", vs. the original plant extract marked as "background"
QC.background.mean = background.mean / BackgroundDiluteFactor,
QC.background.sd = background.sd/BackgroundDiluteFactor, # the original background deviation shrinks after spike-induced dilution
# spiked amount
QC.Spike.Expected = `Stock.conc.ug/mL` / SpikeDiluteFactor * 1000) # converting concentration to ng/mL
# compute expected component-wise concentration at injection
df.inj.conc.expected = df.QC %>%
# remove some redundant columns
select(-contains("Vol.uL")) %>% # remove spike and plant extract volume columns
select(-c(background.mean, background.sd)) %>% # remove original plant extract mean and deviation (prior to spike)
# all three concentration marked as "inj", after 100-fold dilution
mutate(inj.conc.background.mean = QC.background.mean / 100,
inj.conc.background.sd = QC.background.sd / 100,
inj.conc.Spike.Expected = QC.Spike.Expected / 100)
df.inj.conc.expected
# compute measured concentration at injection
df.accuracy = df.inj.conc.accuracy %>%
group_by(compounds, Level) %>%
summarise(conc.inj.mean = mean(conc.inj),
conc.inj.sd = sd(conc.inj)) %>%
# combine the expected level
left_join(df.inj.conc.expected, by = c("compounds", "Level")) %>%
# compute stats summary
mutate(Accuracy = (conc.inj.mean - inj.conc.background.mean) / inj.conc.Spike.Expected * 100,
Accuracy.sd = sqrt(conc.inj.sd^2 + inj.conc.background.sd^2) / inj.conc.Spike.Expected * 100 )
df.accuracy
# Visualize accuracy
dg.Acc = .6 # position_dodge value
errorBarWidth = 1
plt.accuracy = df.accuracy %>% ggplot(aes(x = compounds, y = Accuracy, color = Level)) +
geom_errorbar(aes(ymin = Accuracy - Accuracy.sd,
ymax = Accuracy + Accuracy.sd),
width = errorBarWidth, position = position_dodge(dg.Acc)) +
geom_point(shape = 21, size = 2.5, fill = "white", position = position_dodge(dg.Acc)) +
coord_flip(ylim = c(50, 150)) +
annotate("rect", xmin = .5, xmax = 21.5, ymin = 80, ymax = 120, alpha = .1, fill = "black") +
annotate("segment", x = .5, xend = 21.5, y = 100, yend = 100, linetype = "dashed", size = .4) +
scale_color_brewer(palette = "Dark2")
plt.accuracy
# spike amount vs. background level
plt.spike.background = df.accuracy %>%
mutate(spike.vs.background = inj.conc.Spike.Expected / inj.conc.background.mean) %>%
ggplot(aes(x = spike.vs.background, y = compounds, color = Level)) +
geom_point(shape = 21, size = 2.5, stroke = 1) +
scale_x_log10() + annotation_logticks(side = "b") +
scale_color_brewer(palette = "Dark2")
plt.spike.background
# Accuracy variance vs. (spike amount vs. background) scatter plot
`plt.AccuracyVariance.vs.(spike vs background).scatter` =
df.accuracy %>%
ggplot(aes(x = inj.conc.Spike.Expected / inj.conc.background.mean,
y = Accuracy.sd, color = Level)) +
geom_point(shape = 21, size = 2.5, stroke = 1) +
scale_x_log10() + scale_y_log10() + annotation_logticks() +
scale_color_brewer(palette = "Dark2") +
# accuracy standard deviation line: 10%
geom_segment(aes(x = .1, xend = 30000, y = 20, yend = 20), linetype = "dashed", color = "black", size = .1) +
# 50% spike amount vs background ratio
geom_segment(aes(x = .5, xend = .5, y = .1, yend = 110), linetype = "dashed", color = "black", size = .1) +
theme(legend.position = c(.8, .75), panel.grid = element_blank()) +
geom_text_repel(data = df.accuracy %>% filter(Accuracy.sd > 20),
aes(label = compounds))
`plt.AccuracyVariance.vs.(spike vs background).scatter`
# Accuracy variance vs. (spike amount vs. background) bar plot
`plt.AccuracyVariance.vs.(spike vs background).barplot` =
df.accuracy %>%
ggplot(aes(x = compounds,
y = inj.conc.Spike.Expected / inj.conc.background.mean,
fill = Level, color = Level)) +
geom_bar(stat = "identity", position = position_dodge(.7), alpha = .6) +
scale_color_brewer(palette = "Dark2") +
scale_fill_brewer(palette = "Dark2") +
scale_y_log10() + coord_flip() +
labs(y = "Spike amount vs. background level ratio")
`plt.AccuracyVariance.vs.(spike vs background).barplot`
# Blank measurement contribution to accuracy deviation
plt.accuracy.variance.decomposition = df.accuracy %>%
select(compounds, Level, inj.conc.background.sd, conc.inj.sd) %>%
gather(-c(1:2), key = sd.source, value = sd) %>%
mutate(sd.squared = sd^2) %>%
ggplot(aes(x = compounds, y = sd.squared, fill = sd.source)) +
geom_bar(stat = "identity", position = "fill") +
facet_wrap(~Level, nrow = 1) + coord_flip() +
theme(legend.position = "bottom",
axis.text.x = element_text(angle = 45, vjust = .7),
axis.title.x = element_blank())
plt.accuracy.variance.decomposition
# Matrix effect
df.matrix = df.inj.conc %>% filter(Purpose == "Matrix effect") %>%
gather(-c(Purpose, Sample, Level), key = compounds, value = matrix.conc) %>%
group_by(compounds, Level) %>%
summarise(matrix.conc.mean = mean(matrix.conc, na.rm = T),
matrix.conc.sd = sd(matrix.conc, na.rm = T))
df.matrix
df.matrix = df.accuracy %>%
select(-contains("QC")) %>% # remove QC stats columns to reduce cumbersomeness...
left_join(df.matrix, by = c("compounds", "Level")) %>%
mutate(matrixEffect = (conc.inj.mean - inj.conc.background.mean) / matrix.conc.mean * 100,
matrixEffect.sd =
# use error propogation rule, refer to https://chem.libretexts.org/Courses/Lakehead_University/Analytical_I/4%3A_Evaluating_Analytical_Data/4.03%3A_Propagation_of_Uncertainty
sqrt((conc.inj.sd^2 + inj.conc.background.sd^2) / (conc.inj.mean - inj.conc.background.mean)^2 +
(matrix.conc.sd / matrix.conc.mean)^2 ) * matrixEffect  )
plt.matrixEffect = df.matrix %>%
ggplot(aes(x = compounds, y = matrixEffect, color = Level)) +
geom_errorbar(aes(ymin = matrixEffect - matrixEffect.sd,
ymax = matrixEffect + matrixEffect.sd),
width = errorBarWidth, position = position_dodge(dg.Acc)) +
geom_point(shape = 21, size = 2.5, fill = "white", position = position_dodge(dg.Acc)) +
coord_flip(ylim = c(50, 150)) +
annotate("rect", xmin = .5, xmax = 21.5, ymin = 80, ymax = 120, alpha = .1, fill = "black") +
annotate("segment", x = .5, xend = 21.5, y = 100, yend = 100, linetype = "dashed", size = .4) +
scale_color_brewer(palette = "Dark2")
plt.matrixEffect
# Precision
df.precision = df.inj.conc %>% filter(Purpose == "Precision") %>%
gather(-c(Purpose, Sample, Level), key = compounds, value = precision.conc) %>%
group_by(compounds, Level) %>%
summarise(precision.conc.mean = mean(precision.conc),
precision.conc.sd = sd(precision.conc),
precision = precision.conc.sd / precision.conc.mean * 100)
df.precision
plt.precision = df.precision %>% ggplot(aes(x = compounds, y = precision, color = Level)) +
geom_point(shape = 21, size = 2.5, fill = "white", position = position_dodge(dg.Acc))  +
coord_flip() +
scale_color_brewer(palette = "Dark2")
# Combine accuracy, matrix effect, and precision
plot_grid(
plt.accuracy + theme(legend.position = "NA", axis.title.y = element_blank()),
plt.matrixEffect + theme(
legend.position = "NA", axis.title.y = element_blank(), axis.text.y = element_blank()),
plt.precision + theme(
legend.position = "NA", axis.title.y = element_blank(), axis.text.y = element_blank()),
`plt.AccuracyVariance.vs.(spike vs background).barplot` + theme(
axis.title.y = element_blank(), axis.text.y = element_blank()),
nrow = 1, rel_widths = c(4, 3, 2, 2.5))
# clean up table for publication in supplementary material
df.accuracy.reportTable = df.accuracy %>% select(compounds, Level, Accuracy, Accuracy.sd) %>%
mutate(Accuracy.all = paste(round(Accuracy, 1), "±", round(Accuracy.sd, 1))) %>%
select(-c(Accuracy, Accuracy.sd)) %>% spread(Level, Accuracy.all)
df.accuracy.reportTable
df.matirx.reportTable = df.matrix %>% select(compounds, Level, matrixEffect, matrixEffect.sd) %>%
mutate(matrixEffect = paste(round(matrixEffect, 1), "±", round(matrixEffect.sd, 1))) %>%
select(-matrixEffect.sd) %>% spread(Level, matrixEffect)
df.matirx.reportTable
df.precision.reportTable = df.precision %>% select(compounds, Level, precision) %>%
spread(Level, precision)
df.precision.reportTable
#### <>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>- STABILITY TRACKER  <>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-<>-
df.stability = read_excel(path, sheet = "Stability (Area)")
df.stability.tidy = df.stability %>%
gather(-c(Name, `Data File`, Level), key = compounds, value = stab.conc)
## Add time line
df.stab.time = read_excel(path, sheet = "stability time")
df.stability.tidy = df.stability.tidy %>%
left_join(df.stab.time, by = "Data File") # combine time line with stability dataset
df.stability.tidy = df.stability.tidy %>%
mutate(`Acq. Date-Time.hours` = `Acq. Date-Time` %>% as.numeric(),
# calculate time elapsed (in hour)
hour.elapsed = (`Acq. Date-Time.hours` - min(`Acq. Date-Time.hours`))/3600 ) %>% arrange(hour.elapsed)
## Add injection sequence number
df.stability.tidy$hour.elapsed %>% unique() %>% length() # 44 files (injections)
df.stability.tidy$inj.seq = rep(1:44, each = 21)
## Normalize peak area for each level (relative to the average level)
df.stability.tidy = df.stability.tidy %>%
group_by(compounds, Level) %>%
mutate(remain.frac = stab.conc / mean(stab.conc) * 100)
## Plot degradation profile (injection error analysis)
df.stability.tidy %>%
ggplot(aes(x = hour.elapsed, y = remain.frac, color = compounds)) +
geom_point(position = position_dodge(2), size = .5) +
geom_line(aes(group = compounds), position = position_dodge(2), size = .1) +
geom_text_repel(data = df.stability.tidy %>% filter(remain.frac < 80),
aes(label = compounds, color = compounds), size = 2) +
geom_text_repel(data = df.stability.tidy %>% filter(remain.frac > 115),
aes(label = compounds, color = compounds), size = 2) +
geom_segment(aes(x =0, xend = df.stability.tidy$hour.elapsed %>% max(),
y = 100, yend = 100), size = .3) +
geom_segment(aes(x =0, xend = df.stability.tidy$hour.elapsed %>% max(),
y = 110, yend = 110), size = .2, linetype = "dashed") +
geom_segment(aes(x =0, xend = df.stability.tidy$hour.elapsed %>% max(),
y = 90, yend = 90), size = .2, linetype = "dashed") +
scale_color_manual(values = AA.colors) +
labs(x = "Number of hours elapsed", y = "Remaining fraction",
caption = "Note: Remaining fraction was normalized for each compound-level combination") +
theme(legend.position = "None")
## Plot degradation
df.stability.tidy %>% # filter(inj.seq > 10 ) %>%
ggplot(aes(x = hour.elapsed, y = remain.frac, color = Level)) +
geom_point() + geom_line() +
facet_wrap(~compounds, nrow = 4) +
theme(legend.position = c(.8, .1)) +
scale_color_brewer(palette = "Set1")
## Plot combined response curve and RT shift
plot_grid(plt.acid.response.selected.compounds + theme(legend.position = "bottom"),
plt.acid.RT.diff + theme(legend.position = "bottom"),
nrow = 1, rel_widths = c(.6, .35))  # 16.7 X 8.3
# Combine accuracy, matrix effect, and precision
plot_grid(
plt.accuracy + theme(legend.position = "NA", axis.title.y = element_blank()),
plt.matrixEffect + theme(
legend.position = "NA", axis.title.y = element_blank(), axis.text.y = element_blank()),
plt.precision + theme(
legend.position = "NA", axis.title.y = element_blank(), axis.text.y = element_blank()),
`plt.AccuracyVariance.vs.(spike vs background).barplot` + theme(
axis.title.y = element_blank(), axis.text.y = element_blank()),
nrow = 1, rel_widths = c(4, 3, 2, 2.5))
# Import data and clean up
load("/Users/Boyuan/Desktop/Data Mining/geno.R")
library(tidyverse)
library(glmnet)
library(e1071)
library(randomForest)
library(penalizedSVM)
library(MASS)
set.seed(2019)
# need to put response as factor
# response has k levels, then put reponse into k columns, each column being binary
# but for predictors, k levels put k-1 dummy variables
#
# Import data and clean up
load("/Users/Boyuan/Desktop/Data Mining/geno.R")
x0
x0 = t(x0)
x0
df = cbind(x0==1, x0==2)*1
dim(df)
dim(x0)
df = cbind(y, df)
head(df)
head(df)[, 1]
head(df, n = 72)[, 1]
head(df, n = 73)[, 1]
library(tidyverse)
library(glmnet)
library(e1071)
library(randomForest)
library(penalizedSVM)
library(MASS)
set.seed(2019)
# need to put response as factor
# response has k levels, then put reponse into k columns, each column being binary
# but for predictors, k levels put k-1 dummy variables
#
# Import data and clean up
load("/Users/Boyuan/Desktop/Data Mining/geno.R")
x0 = t(x0)
df = cbind(x0==1, x0==2)*1
df = cbind(y, df) %>% as_tibble()
summary(df)
dim(df)
df$y
df = df %>% mutate(y = as.factor(y))
df[1:10, 1:3]
library(tidyverse)
library(glmnet)
library(e1071)
library(randomForest)
library(penalizedSVM)
library(MASS)
set.seed(2019)
# need to put response as factor
# response has k levels, then put reponse into k columns, each column being binary
# but for predictors, k levels put k-1 dummy variables
#
# Import data and clean up
load("/Users/Boyuan/Desktop/Data Mining/geno.R")
x0 = t(x0)
df = cbind(x0==1, x0==2)*1
df = cbind(y, df) %>% as_tibble()
df = df %>% mutate(y = as.factor(y))
df$y
df$y
df$y %>% as.factor()
df$y
df$y = df$y %>% as.factor()
df$y
# Split train and test data
df.train = df %>% sample_n(57)
nrow(df.train)
df.test = anti_join(df, df.train)
nrow(df.test)
# SVM
mdl.SVM = svm(y ~., data = df.train)
df.train
# Random Forest
mdl.RF = randomForest(x = df.train[, -1], y = df.train[[1]], ntree = 100)
fitted.RF = predict(mdl.RF, newdata = df.test[, -1])
cfTable.RF = table(actual = df.test[[1]], fitted = fitted.RF)
cfTable.RF
sum(df.test[[1]] == fitted.RF) / nrow(df.test)
cfTable.RF
# Glm net
mdl.glm = cv.glmnet(df.train[, -1] %>% as.matrix(), y = df.train[[1]], family = "multinomial", alpha = 1 )
plot(mdl.glm)
fitted.glm = predict(mdl.glm, newx = df.test[, -1] %>% as.matrix(), type = "class", s = mdl.glm$lambda.1se)
cfTable.glm = table(actual = df.test[[1]], fitted = fitted.glm)
cfTable.glm
sum(fitted.glm == df.test[[1]]) / nrow(df.test)
sum(fitted.glm == df.test[[1]]) / nrow(df.test)
sum(df.test[[1]] == fitted.RF) / nrow(df.test)
cfTable.RF
# SVM
mdl.SVM = svm(y ~., data = df.train)
fitted.SVM = predict(mdl.SVM, df.test)
cfTable.SVM = table(actual = df.test[[1]], fitted = fitted.SVM)
cfTable.SVM
sum(df.test[[1]] == fitted.SVM) / nrow(df.test)
df.train[[1]]
library(keras)
d = dataset_mnist()
install_tensorflow()
install_tensorflow()
install_tensorflow()
library(keras)
install_tensorflow()
devtools::install_github("rstudio/keras")
library(keras)
install_keras()
library(tensorflow)
d = dataset_mnist()
dataset_mnist()
dataset_mnist()
library(tensorflow)
dataset_mnist()
tensorflow::install_tensorflow()
source r-tensorflow/bin/activate
use_condaenv("r-tensorflow")
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
install_tensorflow()
install_tensorflow()
pip install --upgrade tensorflow-gpu==2.0.0-rc0
library(keras)
install_keras()
set.seed(123)
df <- data.frame(a=rnorm(1000),
b=rnorm(1000),
c=rnorm(1000))
y=ifelse(df$a>-.075 & df$b<.05 & df$c>0,
sample(c(0,1),1,prob=c(.1,.9)),
sample(c(1,0),1,prob=c(.1,.9)))
df <- cbind(y,df)
#split into train/test
test_ind <- seq(1, length(df$y), 5)
train_x <- data.matrix(df[-test_ind,-1])
train_y <- data.matrix(df[-test_ind, 1])
test_x <-  data.matrix(df[test_ind,-1])
test_y <-  data.matrix(df[test_ind, 1])
library(keras)
# set keras seed
use_session_with_seed(345)
# defining a keras sequential model
model <- keras_model_sequential()
# model architecture
model %>%
layer_dense(units = 20, input_shape = ncol(train_x)) %>%
layer_dropout(rate=0.25)%>%
layer_activation(activation = 'relu') %>%
layer_dense(units = 1) %>%
layer_activation(activation = 'sigmoid')
# compiling the defined model with metric = accuracy and optimiser as adam.
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# fitting the model on the training dataset
model %>% fit(train_x, train_y,
epochs = 1000)
# score and produce predictions
score <- model %>% evaluate(test_x, test_y)
Sys.setenv(RETICULATE_PYTHON = "/anaconda3/envs/r-tensorflow/bin")
dataset_mnist()
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/keras")
tensorflow::install_tensorflow()
tensorflow::tf_config()
reticulate::py_discover_config()
reticulate::use_condaenv("r-tensorflow")
reticulate::py_config()
library(keras)
install_tensorflow()
install_keras()
install.packages(c("AlgDesign", "Amelia", "BiocManager", "bookdown", "callr", "car", "carData", "caTools", "cli", "covr", "curl", "data.table", "dendextend", "Deriv", "DescTools", "deSolve", "digest", "DT", "e1071", "effects", "Epi", "exactRankTests", "factoextra", "FactoMineR", "farver", "fields", "forecast", "fracdiff", "geepack", "gganimate", "ggpmisc", "ggpubr", "glmnet", "hexbin", "HH", "htmlTable", "igraph", "kernlab", "knitr", "leaflet", "leafpop", "maptools", "Matrix", "MCMCpack", "memisc", "miscTools", "mlogit", "multcomp", "network", "nlme", "numbers", "OpenMx", "openxlsx", "optmatch", "pingr", "plyr", "prodlim", "quadprog", "R.cache", "R.utils", "RandomFields", "Rcmdr", "RcppArmadillo", "RcppEigen", "rgdal", "RgoogleMaps", "rmarkdown", "rms", "roxygen2", "RSQLite", "rversions", "satellite", "scales", "segmented", "selectr", "shinyWidgets", "spam", "spam64", "survival", "systemfit", "testit", "testthat", "tidyverse", "tinytex", "VGAM", "visNetwork", "webshot", "xfun"))
install.packages(c("AlgDesign", "Amelia", "BiocManager", "bookdown", "callr", "car", "carData", "caTools", "cli", "covr", "curl", "data.table", "dendextend", "Deriv", "DescTools", "deSolve", "digest", "DT", "e1071", "effects", "Epi", "exactRankTests", "factoextra", "FactoMineR", "farver", "fields", "forecast", "fracdiff", "geepack", "gganimate", "ggpmisc", "ggpubr", "glmnet", "hexbin", "HH", "htmlTable", "igraph", "kernlab", "knitr", "leaflet", "leafpop", "maptools", "Matrix", "MCMCpack", "memisc", "miscTools", "mlogit", "multcomp", "network", "nlme", "numbers", "OpenMx", "openxlsx", "optmatch", "pingr", "plyr", "prodlim", "quadprog", "R.cache", "R.utils", "RandomFields", "Rcmdr", "RcppArmadillo", "RcppEigen", "rgdal", "RgoogleMaps", "rmarkdown", "rms", "roxygen2", "RSQLite", "rversions", "satellite", "scales", "segmented", "selectr", "shinyWidgets", "spam", "spam64", "survival", "systemfit", "testit", "testthat", "tidyverse", "tinytex", "VGAM", "visNetwork", "webshot", "xfun"))
install.packages(c("exactRankTests", "geepack", "glmnet", "pingr", "RandomFields", "spam", "spam64"))
?tensorflow::dict()
library(keras)
install_keras()
mnist <- dataset_mnist()
dataset_mnist()
View(train_y)
remove.packages("tensorflow")
remove.packages("tfruns")
remove.packages("keras")
