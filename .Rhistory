x = coef(cv.mdl.logistic.elasticNet, s = "lambda.min")
df.logisticNets.coefficients =
data.frame(x[[1]] %>% as.matrix(), x[[2]] %>% as.matrix(),
x[[3]] %>% as.matrix(), x[[4]] %>% as.matrix())
colnames(df.logisticNets.coefficients) = names(x)
df.logisticNets.coefficients
# Prediction with train-test split, a formal test of model efficiency
# Define function for performing regularized logistic with different alpha values
func.regularizedLogistic = function(
input.alpha, # control ridge, lasso, or between
ModelName    # model type as extra column note in the confusion table output
){
# train with 10-fold cross validation
cv.mdl.logistic = cv.glmnet(x = df.train[, -1] %>% as.matrix(), y = df.train$Category,
family = "multinomial", alpha = input.alpha, nfolds = 10)
# predict with test set
fitted.logistic =
predict(cv.mdl.logistic, newx = df.test[, -1] %>% as.matrix(),
s = cv.mdl.logistic$lambda.1se, type = "class") %>% c() %>%
factor(levels = sort(unique.categories), ordered = T) # Note: important to sort unique.categories!
# wrap up prediction results
cf.logistic = confusionMatrix(data = fitted.logistic, reference = df.test$Category)
cf.counts.logistic = cf.logistic$table %>% func.tidy.cf.contigencyTable(ModelName = ModelName)
cf.stats.logistic = cf.logistic$byClass %>% func.tidy.cf.statsTable(ModelName = ModelName)
return(list(cf.counts.logistic, cf.stats.logistic, fitted.logistic))
}
# Test upon different alpha values (important to note that alpha is not a hyper-parameter to optimize!)
func.regularizedLogistic(input.alpha = 0, ModelName = "Ridge")
func.regularizedLogistic(input.alpha = 1, ModelName = "Lasso")
func.regularizedLogistic(input.alpha = 0.5, ModelName = paste("ElasticNet, α = 0.5") )
# We'remore interested in the elastic net results
cf.counts.ElasticNet = func.regularizedLogistic(input.alpha = 0.5, ModelName = paste("Elastic Net, α = 0.5") )[[1]]
cf.stats.ElasticNet = func.regularizedLogistic(input.alpha = 0.5, ModelName = paste("Elastic Net, α = 0.5") )[[2]]
# Classification and regression tree (CART) -----------------------------------------------------------
# A preliminary run
colnames(df.train) = make.names(colnames(df.train))
cv.mdl.CART <- train(
Category ~., data = df.train, method = "rpart",
trControl = trainControl("cv", number = 30),
tuneLength = 20)
# Plot cross-validation-tuned model complexity parameter
plot(cv.mdl.CART)
cv.mdl.CART$bestTune # very small penalty imposed after cross validation
# Plot decision tree
# plot(cv.mdl.CART$finalModel)
# par(xpd = NA) # avoid text from being clipped
# text(cv.mdl.CART$finalModel, font = 2, col = "firebrick")
# A more polished version
# plot the training dataset learnt results
prp(cv.mdl.CART$finalModel, type = 5, extra = 2,
fallen.leaves = T,
border.col = color.category, shadow.col = color.category,
branch.col = "black", varlen = 0) # spell out all names
# The classification tree works identical with unscaled data, generating same prediction result, but when plotted, the split criteria is more directly interpretable than scaled data.
# Prediction with train-test split, a formal test of model efficiency-----
colnames(df.test) = make.names(colnames(df.test))
# train
cv.mdl.CART = train(Category ~ ., data = df.train, method = "rpart",
trControl = trainControl("cv", number = 10),
tuneLength = 20)
# predict
fitted.CART = predict(cv.mdl.CART, newdata = df.test)
cf.CART = confusionMatrix(data = fitted.CART, reference = df.test$Category, mode = "everything")
# confusion matirx
cf.counts.CART = cf.CART$table %>% func.tidy.cf.contigencyTable(ModelName = "Classification tree")
# summary stats
cf.stats.CART = cf.CART$byClass %>% func.tidy.cf.statsTable(ModelName = "Classification tree")
# Random forest -----------------
colnames(df.train) = make.names(colnames(df.train))
# fine tune mtry (# features randomly selected for each split)
sumErrors.percent = c() # collect error ratio for each mtry value
# loop through different mtry (# of features randomly select and compare for each split)
mtry = 2:ncol(df.train[, -1])
for ( i in mtry ) {
mdl.randomForest = randomForest(Category ~., data = df.train, mtry = i, ntree = 500)
# predict on the same training set as quick accuracy test
# Considering that a third dataset not used due to bootstrap
fitted = mdl.randomForest$predicted %>% factor(levels = unique.categories, ordered = T)
sumErrors.i = (! fitted == df.train$Category ) %>% sum()
sumErrors.percent.i = sumErrors.i / nrow(df.train) * 100
sumErrors.percent = append(sumErrors.percent, sumErrors.percent.i)
}
sumErrors.percent
plot(x = mtry, y = sumErrors.percent, pch = 19,
xlab = "Number of features to randomly select for each split",
ylab = "Error percentage (%)",
main = "Random forest hyperparameter tuning")
lines(x = mtry, y = sumErrors.percent)
# Five variables to select from renders the highest accuracy (another random shuffle could give a different answer)
# Overally, the more variables the lower prediction accuracy
# Decide to use four variables as mtry parameter value, which is also reasonably close to square root of total feature numbers
# Predict with train-test split
colnames(df.train) = make.names(colnames(df.train))
colnames(df.test) = make.names(colnames(df.test))
# set up training model and test accuracy
mdl.randomForest = randomForest(Category ~., data = df.train, ntree = 500, mtry = 5) # four features to ramdo
fitted.randomForest = predict(mdl.randomForest, newdata = df.test)
# set up confusion table
cf.randomForest = confusionMatrix(data = fitted.randomForest,
reference = df.test$Category,
mode = "everything")
# confusion matrix
cf.counts.randomForest = cf.randomForest$table %>%
func.tidy.cf.contigencyTable(ModelName = "Random forest")
# Summary stats
cf.stats.randomForest = cf.randomForest$byClass %>%
func.tidy.cf.statsTable(ModelName = "Random forest")
cf.counts.randomForest
cf.stats.randomForest
# Support vector machine
# Cross validation to tune hyper-parameters
svm.gamma = 10^(seq(-5, 2, by = 1))
svm.cost = 10^(seq(-2, 5, by = 1))
cv.svm = df.train %>%
vfold_cv(strata = Category, v = 5) %>%
mutate(train = map(.x = splits, .f = ~training(.x) ),
validate = map(.x = splits, .f = ~testing(.x) ))  %>%
select(-splits) %>%
crossing(gamma = svm.gamma, cost = svm.cost) %>%
mutate(hyper = map2(.x = gamma, .y = cost, .f = ~list(.x, .y))) %>% # 1st gamma; 2nd cost
# set up model CV tuning hyper-params
mutate(model = map2(.x = train, .y = hyper,
.f = ~svm(x = .x[, -1], y = .x[[1]],  gamma = .y[[1]], cost = .y[[2]] ) )) %>%
# predict on validation set
# note that here the CV is performed in a somewhat loose manner as the train-validation folds have been normalized upstream prior to train-validation split
mutate(fitted.validate = map2(.x = model, .y = validate,
.f = ~predict(.x,  newdata = .y[, -1] )),
actual.validate = map(.x = validate, .f = ~.x[[1]] %>% factor(ordered = F)),
accuracy = map2_dbl(.x = fitted.validate , .y = actual.validate,
.f = ~sum(.x == .y)/length(.x)))
cv.svm.summary = cv.svm %>%
group_by(gamma, cost) %>%
summarise(accuracy.mean = mean(accuracy) * 100,
accuracy.sd = sd(accuracy) * 100 ) %>%
arrange(desc(accuracy.mean))
cv.svm.summary %>%
ggplot(aes(x = gamma, y = cost, z = accuracy.mean)) +
geom_tile(aes(fill = accuracy.mean)) +
scale_fill_viridis(option = "A", alpha = .9)  +
# stat_contour(color = "grey", size = .5) +
coord_fixed() +
theme(panel.grid.minor = element_line(colour = "black", size = 2),
panel.grid.major = element_blank()) +
scale_x_log10(breaks = svm.gamma, labels = log10(svm.gamma)) +
scale_y_log10(breaks = svm.cost, labels = log10(svm.cost)) +
labs(x = "gamma, 10 ^ X", y = "cost, 10 ^ X", title = "SVM Radial Kernel") +
geom_text(aes(label = accuracy.mean %>% round(1) ), color = "black")
cv.svm.summary
# train models using entire trainingset and optimized hyper-params
mdl.svm = svm(x = df.train[, -1], y = df.train$Category,
gamma = cv.svm.summary$gamma[1],
cost = cv.svm.summary$cost[1])
# predict
fitted.svm = predict(mdl.svm, newdata = df.test[, -1])
# confusion matrix
cf.svm = confusionMatrix(
data = fitted.svm, reference = df.test$Category, mode = "everything")
# confusion matrix
cf.counts.svm = cf.svm$table %>%
func.tidy.cf.contigencyTable(ModelName = "Support vector machine")
# summary stats
cf.stats.svm = cf.svm$byClass %>%
func.tidy.cf.statsTable(ModelName = "Support vector machine")
# naiveBayes as the bentop mark
# train
mdl.Bayes = naiveBayes(x = df.train[, -1], y = df.train$Category)
# predict
fitted.Bayes = predict(mdl.Bayes, newdata = df.test, type = "class")
# confusion matrix
cf.Bayes = confusionMatrix(
data = fitted.Bayes, reference = df.test$Category, mode = "everything")
# confusion matrix
cf.counts.Bayes = cf.Bayes$table %>%
func.tidy.cf.contigencyTable(ModelName = "Naive Bayes")
# summary stats
cf.stats.Bayes = cf.Bayes$byClass %>%
func.tidy.cf.statsTable(ModelName = "Naive Bayes")
# Test results
## Wrap up
unique.models = factor(c("Linear discriminant analysis", "Quadratic discriminant analysis", "Elastic net", "Random forest", "Support vector machine", "Naive Bayes"))
# Summary of all machine learning techniques
df.confusionMatrix.all = cf.counts.LDA %>% rbind(cf.counts.QDA) %>%
rbind(cf.counts.ElasticNet) %>% # rbind(cf.counts.CART) %>%
rbind(cf.counts.randomForest) %>% rbind(cf.counts.svm) %>%
rbind(cf.counts.Bayes) %>% as_tibble()
# tidy up the confusion matrix combined
df.confusionMatrix.all.tidy = df.confusionMatrix.all %>%
# tidy up
gather(-c(Prediction, Model), key = reference, value = counts) %>%
# convert AIVs category into ordered factor
mutate(reference = factor(reference, levels = unique.categories, ordered = T),
Prediction = factor(Prediction, levels = unique.categories %>% rev(), ordered = T)) %>%
# change model order in the dataset
mutate(Model = factor(Model, levels = unique.models, ordered = T)) %>%
arrange(Model, reference, Prediction) %>%
mutate(Diaganol = Prediction == reference)
## Plot confusion matrix
# Assign color to correct / incorrect prediction
diag = df.confusionMatrix.all.tidy %>%
filter(Diaganol == F)
df.confusionMatrix.all.tidy = diag %>%
mutate(color = ifelse(counts == 0, "Grey", "Firebrick")) %>%
rbind(df.confusionMatrix.all.tidy %>% filter(Diaganol == T) %>%
mutate(color = "steelblue"))
df.confusionMatrix.all.tidy %>%
ggplot(aes(x = reference, y = Prediction)) +
facet_wrap(~Model, nrow = 2) +
# off diaganol incorrect prediction
geom_label(data = df.confusionMatrix.all.tidy %>% filter(color == "Firebrick"),
aes(label = counts),
fill = "firebrick", alpha = .3, size = 6) +
# diaganol correct prediction
geom_label(data = df.confusionMatrix.all.tidy %>% filter(color == "steelblue"),
aes(label = counts),
fill = "Steelblue", alpha = .3, size = 6) +
# zero counts
geom_label(data = df.confusionMatrix.all.tidy %>% filter(color == "Grey"),
aes(label = counts),
size = 6, color = "grey")  +
theme_bw() +
theme(axis.text.x = element_text(angle = 30, vjust = .8, hjust = .8),
strip.background = element_blank(),
strip.text = element_text(face = "bold"),
panel.border = element_rect(color = "black", size = 1),
title = element_text(face = "bold")) +
ggtitle("Confusion Matrix") +
labs(x = "\nReference", y = "Prediction\n") +
coord_fixed(ratio = 1)
## Sample-wise comparison among models
# Sample-wise comparison between models
fitted.ElasticNet = func.regularizedLogistic(input.alpha = 0.5, ModelName = "Elastic Net")[[3]]
df.actual.vs.fit = data.frame(
"Actual" = df.test$Category,
"Linear discriminant" = fitted.LDA,
"Quadratic discriminant" = fitted.QDA,
"Elastic net" = fitted.ElasticNet,
# "CART" = fitted.CART,
"Random forest" =  fitted.randomForest,
"Support vector machine" =  fitted.svm,
"Naive Bayes" = fitted.Bayes)
df.actual.vs.fit = df.actual.vs.fit %>% as_tibble() %>%
mutate(Actual = factor(Actual, ordered = F))
df.actual.vs.fit
# most votes
func.mostVotes = function(vector){
x = vector %>% table() %>% sort() %>% rev()
names(x)[1] %>% return()
}
df.actual.vs.fit = df.actual.vs.fit %>%
mutate('Most voted' =  apply(df.actual.vs.fit %>% select(-Actual),
MARGIN = 1, func.mostVotes))
# Heatmap of sample-wise predicted result
plt.heatmap.machineLearning =
df.actual.vs.fit %>%
as.matrix() %>%
Heatmap(col = color.category,
heatmap_legend_param = list(
title = "Category", title_position = "leftcenter",
nrow = 1,
labels_gp = gpar(fontsize = 11)),
rect_gp = gpar(col = "white", lwd = 0.1))
draw(plt.heatmap.machineLearning,
heatmap_legend_side = "bottom")
# Individual conditional expectation analysis
func.plot.ICE = function(feature){
d.ICE = cbind(code = 1:nrow(df.train), df.train) %>% as_tibble()
feature = "leucine"
grid.sequence =  seq(from = df.train[[feature]] %>% min(),
to = df.train[[feature]] %>% max(),
length.out =100)
d.ICE = expand.grid(code = df.train$code, grid = grid.sequence) %>%
left_join(df.train, by = "code") %>%
as_tibble()
d.ICE[[feature]] = d.ICE$grid
d.ICE = d.ICE %>% select(-grid)
d.prob = predict(mdl.randomForest, newdata = d.ICE, type = "prob") %>%
as_tibble() %>%
mutate(actual = d.ICE$Category,
code = d.ICE$code,
grid = d.ICE[[feature]])
d.prob.tidy = d.prob %>%
gather(1:4, key = fitted, value = prob)
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category)
}
func.plot.ICE(feature = "leucine")
train
colnames(df.train)
colnames(df.train)[-1]
colnames(df.train)[-1]
colnames(df.train)[-1]
for (a in colnames(df.train)[-1] )
func.plot.ICE(feature = a)
for (a in colnames(df.train)[-1] ) {
func.plot.ICE(feature = a)
}
colnames(df.train)[-1]
for (a in colnames(df.train)[-1] ) {
func.plot.ICE(feature = a)
}
# Individual conditional expectation analysis
func.plot.ICE = function(feature){
d.ICE = cbind(code = 1:nrow(df.train), df.train) %>% as_tibble()
feature = "leucine"
grid.sequence =  seq(from = df.train[[feature]] %>% min(),
to = df.train[[feature]] %>% max(),
length.out =100)
d.ICE = expand.grid(code = df.train$code, grid = grid.sequence) %>%
left_join(df.train, by = "code") %>%
as_tibble()
d.ICE[[feature]] = d.ICE$grid
d.ICE = d.ICE %>% select(-grid)
d.prob = predict(mdl.randomForest, newdata = d.ICE, type = "prob") %>%
as_tibble() %>%
mutate(actual = d.ICE$Category,
code = d.ICE$code,
grid = d.ICE[[feature]])
d.prob.tidy = d.prob %>%
gather(1:4, key = fitted, value = prob)
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category)
}
func.plot.ICE(feature = "leucine")
feature = "leucine"
d.ICE = cbind(code = 1:nrow(df.train), df.train) %>% as_tibble()
feature = "leucine"
grid.sequence =  seq(from = df.train[[feature]] %>% min(),
to = df.train[[feature]] %>% max(),
length.out =100)
d.ICE = expand.grid(code = df.train$code, grid = grid.sequence) %>%
left_join(df.train, by = "code") %>%
as_tibble()
expand.grid
expand.grid(code = df.train$code, grid = grid.sequence)
expand.grid(code = df.train$code, grid = grid.sequence) %>%
left_join(df.train, by = "code")
df.train
d.ICE = cbind(code = 1:nrow(df.train), df.train) %>% as_tibble()
feature = "leucine"
grid.sequence =  seq(from = df.train[[feature]] %>% min(),
to = df.train[[feature]] %>% max(),
length.out =100)
d.ICE = expand.grid(code = df.train$code, grid = grid.sequence) %>%
left_join(d.ICE, by = "code") %>%
as_tibble()
expand.grid
expand.grid(code = df.train$code, grid = grid.sequence)
d.ICE = cbind(code = 1:nrow(df.train), df.train) %>% as_tibble()
d.ICE
feature = "leucine"
grid.sequence =  seq(from = df.train[[feature]] %>% min(),
to = df.train[[feature]] %>% max(),
length.out =100)
grid.sequence
expand.grid(code = df.train$code, grid = grid.sequence)
expand.grid(code = df.train$code, grid = grid.sequence)
df.train
expand.grid(code = d.ICE$code, grid = grid.sequence)
d.ICE = cbind(code = 1:nrow(df.train), df.train) %>% as_tibble()
feature = "leucine"
d.ICE = cbind(code = 1:nrow(df.train), df.train) %>% as_tibble()
feature = "leucine"
grid.sequence =  seq(from = d.ICE[[feature]] %>% min(),
to = d.ICE[[feature]] %>% max(),
length.out =100)
d.ICE = expand.grid(code = d.ICE$code, grid = grid.sequence) %>%
left_join(d.ICE, by = "code") %>%
as_tibble()
d.ICE[[feature]] = d.ICE$grid
d.ICE = d.ICE %>% select(-grid)
d.prob = predict(mdl.randomForest, newdata = d.ICE, type = "prob") %>%
as_tibble() %>%
mutate(actual = d.ICE$Category,
code = d.ICE$code,
grid = d.ICE[[feature]])
d.prob = predict(mdl.randomForest, newdata = d.ICE, type = "prob") %>%
as_tibble() %>%
mutate(actual = d.ICE$Category,
code = d.ICE$code,
grid = d.ICE[[feature]])
d.prob.tidy = d.prob %>%
gather(1:4, key = fitted, value = prob)
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category)
# Individual conditional expectation analysis
func.plot.ICE = function(feature){
d.ICE = cbind(code = 1:nrow(df.train), df.train) %>% as_tibble()
grid.sequence =  seq(from = d.ICE[[feature]] %>% min(),
to = d.ICE[[feature]] %>% max(),
length.out =100)
d.ICE = expand.grid(code = d.ICE$code, grid = grid.sequence) %>%
left_join(d.ICE, by = "code") %>%
as_tibble()
d.ICE[[feature]] = d.ICE$grid
d.ICE = d.ICE %>% select(-grid)
d.prob = predict(mdl.randomForest, newdata = d.ICE, type = "prob") %>%
as_tibble() %>%
mutate(actual = d.ICE$Category,
code = d.ICE$code,
grid = d.ICE[[feature]])
d.prob.tidy = d.prob %>%
gather(1:4, key = fitted, value = prob)
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category)
}
func.plot.ICE(feature = "leucine")
for (a in colnames(df.train)[-1] ) {
func.plot.ICE(feature = a)
}
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category)
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle("dfs")
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 14, face = "bold"))
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5))
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5)) +
geom_rug(data = df.train, aes_string(x = feature))
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5)) +
geom_rug(data = df.train, aes_string(x = feature), inherit.aes = F)
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5)) +
geom_rug(data = df.train, aes_string(x = feature), inherit.aes = F, alpha = .2)
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5)) +
geom_rug(data = df.train, aes_string(x = feature), inherit.aes = F, alpha = .1)
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5)) +
geom_rug(data = df.train, aes_string(x = feature), inherit.aes = F, alpha = .1, length = unit(2, "lines"))
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5)) +
geom_rug(data = df.train, aes_string(x = feature), inherit.aes = F, alpha = .1, length = unit(.5, "lines"))
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5)) +
geom_rug(data = df.train, aes_string(x = feature), inherit.aes = F, alpha = .1, length = unit(.25, "lines"))
d.prob.tidy %>%
ggplot(aes(x = grid, y = prob, color = actual)) +
geom_line(aes(group = code), alpha = .3) +
facet_wrap(~fitted, nrow = 1) +
scale_color_manual(values = color.category) +
ggtitle(feature) +
theme(plot.title = element_text(size = 16, face = "bold", hjust = .5)) +
geom_rug(data = df.train, aes_string(x = feature), inherit.aes = F, alpha = .1, length = unit(.3, "lines"))
names = colnames(df.train)[-1]
df.train
